<!doctype html>
<html>

<head>
  <title>Food Analyzer</title>
</head>

<script src="./audiobuffer-to-wav.js"> </script>

<script async defer>
  let recording = false;
  let analyser;
  const clipSecs = 2;
  const inferencesPerSecond = 2;
  let audioContext;
  let oneSecChunks = []

  let ws

  ws = new WebSocket(`ws://localhost:8765`);

  ws.onerror = (e) => {
    console.error(e);
    const errorNode = document.getElementById('error');
    errorNode.classList.remove('invisible');
  }

  ws.onmessage = (msg) => {
    const result = document.getElementById('result')
    console.log(`Got message: `, msg.data)
    const prediction = msg.data.split(':')[1]
    result.innerText = `Prediction: ${prediction}`

  }


  // Visualize the mic's input on the HTML canvas
  function visualize() {
    const canvas = document.getElementById('visualizer');
    const canvasContext = canvas.getContext('2d');
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    analyser.getByteFrequencyData(dataArray);

    canvasContext.clearRect(0, 0, canvas.width, canvas.height);
    const barWidth = (canvas.width / bufferLength) * 2.5;
    let barHeight;
    let x = 0;

    for (let i = 0; i < bufferLength; i++) {
      barHeight = dataArray[i] / 2;

      canvasContext.fillStyle = `rgb(50,${(barHeight + 100)},50)`;
      canvasContext.fillRect(x, canvas.height - barHeight, barWidth, barHeight);

      x += barWidth + 1;
    }
    if (recording)
      requestAnimationFrame(visualize);
    else
      canvasContext.clearRect(0, 0, canvas.width, canvas.height);
  }

  // Start listening to the mic and inferring food classes
  async function startRecording() {
    const stream = await navigator.mediaDevices
      .getUserMedia({
        audio: true,
      },
      );
    audioContext = new AudioContext();
    const source = audioContext.createMediaStreamSource(stream);
    const mediaRecorder =
      new MediaRecorder(source.mediaStream);
    analyser = audioContext.createAnalyser();
    source.connect(analyser)
    analyser.fftSize = 256;
    analyser.smoothingTimeConstant = 0.8;
    mediaRecorder.ondataavailable = handleAudioData;
    visualize();
    while (recording) {
      mediaRecorder.start();
      await sleep(clipSecs * 1000 / inferencesPerSecond);
      mediaRecorder.stop();
    }
    mediaRecorder.stop();
    audioContext.close();
  }

  // Asynchronously sleep for `ms` amount of miliseconds
  async function sleep(ms) {
    return new Promise((resolve) => {
      setTimeout(() => resolve(), ms)
    })
  }

  // Audio chunk received. Decode it into PCM, concatenate it to the chunks already have, 
  // and if we have a full clip ready for inference, convert it to WAV and send it to the server
  async function handleAudioData(event) {
    const arrayBuffer = await event.data.arrayBuffer();
    const chunk = await audioContext.decodeAudioData(arrayBuffer);

    // We might accumulate chunks faster than we can process them. If this happens, we will get garbage audio.
    // To prevent this, we'll wait if the buffer is full.
    while (oneSecChunks.length >= clipSecs * inferencesPerSecond)
      await sleep(10)

    oneSecChunks.push(chunk);
    if (oneSecChunks.length >= clipSecs * inferencesPerSecond) {
      const audioClip = concatBuffer(oneSecChunks);
      oneSecChunks.shift();
      const wav = audioBufferToWav(audioClip);
      ws.send(wav);
    }
  }

  // User pressed the stop button. Stop listening to mic, throw away all the accumulated chunks.
  function stopRecording() {
    recording = false;
    oneSecChunks = [];
  }

  // Concatenate two audio buffers
  function concatBuffer(buffers) {
    var buflength = buffers.length;
    var channels = [];
    var totalSamples = 0;
    for (var a = 0; a < buflength; a++) {
      totalSamples += buffers[a].length;
    }

    var numberOfChannels = 1
    var tmp = audioContext.createBuffer(numberOfChannels, totalSamples, audioContext.sampleRate);// Create new buffer
    var channel = tmp.getChannelData(0);
    var dataIndex = 0;
    for (var c = 0; c < buflength; c++) {
      channel.set(buffers[c].getChannelData(0), dataIndex);
      dataIndex += buffers[c].length;// Next position where we should store the next buffer values
    }

    return tmp;
  }

  function handleClick() {
    const btn = document.getElementById("startBtn");
    if (recording) {
      recording = false;
      btn.innerText = 'Start recording'
      stopRecording();
    } else {
      recording = true;
      btn.innerText = 'Stop recording'
      startRecording();
    }
  }

</script>

<body>
  <div class="container">
    <button id="startBtn" onclick="handleClick()">Start recording</button>

    <canvas id="visualizer" width="300" height="150"></canvas>

    <div id="result"></div>

    <div id="error" class="invisible">Error: Could not connect to WebSocket. Is the project running locally? </div>
  </div>
</body>


<style>
  #startBtn {
    width: 10rem;
    height: 5rem;
  }

  #visualizer {
    width: 300px;
    height: 150px;
    margin-top: 1rem;
    background-color: gainsboro;
  }

  .container {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
  }

  #error {
    color: rgb(175, 3, 3);
    font-size: 1.5rem;
    background-color: pink;
    padding: 0.5rem;
  }

  #result {
    font-family: Arial, Helvetica, sans-serif;
    font-size: 2.5rem;
    margin-top: 1rem;
  }

  .invisible {
    visibility: hidden;
  }
</style>

</html>